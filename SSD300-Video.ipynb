{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\issam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\issam\\Documents\\ssd_keras\\keras_layers\\keras_layer_DecodeDetections.py:174: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\issam\\Documents\\ssd_keras\\keras_loss_function\\keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline\n",
    "img_height = 300\n",
    "img_width = 300\n",
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, 3),\n",
    "                n_classes=20,\n",
    "                mode='inference',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=[0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05], # The scales for MS COCO are [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]\n",
    "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5]],\n",
    "                two_boxes_for_ar1=True,\n",
    "                steps=[8, 16, 32, 64, 100, 300],\n",
    "                offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "                clip_boxes=False,\n",
    "                variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                normalize_coords=True,\n",
    "                subtract_mean=[123, 117, 104],\n",
    "                swap_channels=[2, 1, 0],\n",
    "                confidence_thresh=0.5,\n",
    "                iou_threshold=0.45,\n",
    "                top_k=200,\n",
    "                nms_max_output_size=400)\n",
    "\n",
    "# 2: Load the trained weights into the model.\n",
    "\n",
    "# TODO: Set the path of the trained weights.\n",
    "weights_path = 'VGG_VOC0712Plus_SSD_300x300_ft_iter_160000.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Compile the model so that Keras won't complain the next time you load it.\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "464\n",
      "607.9129197597504\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "import argparse\n",
    "import datetime\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "vs = cv2.VideoCapture('../modd.avi')\n",
    "# initialize the first frame in the video stream\n",
    "firstFrame = None\n",
    "frameCount =0\n",
    "inferencesCount=0\n",
    "inferencesCountFinal=0\n",
    "framesCountFinal=0\n",
    "frame_width = int(vs.get(3))\n",
    "frame_height = int(vs.get(4))\n",
    "print(frame_width)\n",
    "print(frame_height)\n",
    "out = cv2.VideoWriter('full_baseline.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 30, (frame_width,frame_height))\n",
    "started = False\n",
    "# your code\n",
    "no_tracking_res= []\n",
    "while frameCount<641:\n",
    "    # grab the current frame and initialize the occupied/unoccupied\n",
    "    # text\n",
    "    frame = vs.read()[1]\n",
    "    \n",
    " \n",
    "    # if the frame could not be grabbed, then we have reached the end\n",
    "    # of the video\n",
    "    if frame is None:\n",
    "        break\n",
    "    frameCount+=1\n",
    "    if(frameCount<0):\n",
    "        continue\n",
    "    elif started==False:\n",
    "        start_time = time.time()\n",
    "        started=True\n",
    "    # resize the frame, convert it to grayscale, and blur it\n",
    "    cv2.imwrite('temp.jpg',frame)\n",
    "    img = image.load_img('temp.jpg', target_size=(img_height, img_width))\n",
    "    img = image.img_to_array(img)\n",
    "    input_images = []\n",
    "    \n",
    "    input_images.append(img)\n",
    "    input_images = np.array(input_images)\n",
    "    y_pred = model.predict(input_images)\n",
    "    \n",
    "    confidence_threshold = 0.4\n",
    "\n",
    "    y_pred_thresh = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n",
    "    # loop over the contours\n",
    "\n",
    "    for box in y_pred_thresh[0]:\n",
    "    # Transform the predicted bounding boxes for the 512x512 image to the original image dimensions.\n",
    "        if(box[0]!=4):\n",
    "            continue\n",
    "        xmin = int(box[-4] * frame.shape[1] / img_width)\n",
    "        ymin = int(box[-3] * frame.shape[0] / img_height)\n",
    "        xmax =int(box[-2] * frame.shape[1] / img_width)\n",
    "        ymax =int(box[-1] * frame.shape[0] / img_height)\n",
    "        cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax),int(ymax)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame,'{}: {:.2f}'.format(classes[int(box[0])], box[1]), (xmin, ymin),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        no_tracking_res.append({\"image_id\" : frameCount, \"category_id\" : 1, \"bbox\" : [float(xmin),float(ymin),float(xmax-xmin),float(ymax-ymin)], \"score\" : int(box[1])})\n",
    "   \n",
    "    out.write(frame)\n",
    "# cleanup the camera and close any open windows\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n",
    "out.release()\n",
    "with open('full_baseline.json', 'w') as outfile:  \n",
    "    json.dump(no_tracking_res, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('1_no_tracking.json', 'w') as outfile:  \n",
    "    json.dump(no_tracking_res, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes_match(box1,box2,iou_threshold=0.4):\n",
    "    \n",
    "    if(box1[1]!=box2[1] or box1[7]==1 or box2[7]==1):\n",
    "        return False\n",
    "    \n",
    "    return iou(box1[3:7],box2[3:7])>0.4\n",
    "\n",
    "def boxes_equal(box1,box2,iou_threshold=0.6):\n",
    "   \n",
    "    if(box1[1]!=box2[1] or box1[2]!=box2[2]):\n",
    "        return False\n",
    "    \n",
    "    return iou(box1[3:7],box2[3:7])==1\n",
    "def box_matches_list(box1,list_boxes):\n",
    "    index =-1\n",
    "    if(len(list_boxes)==0):\n",
    "        return index\n",
    "    match_list = []\n",
    "    box_index =0\n",
    "    \n",
    "    for box in list_boxes:\n",
    "       \n",
    "        if(box.shape[0]>0):\n",
    "            \n",
    "            if(box.shape[0]==1):\n",
    "                \n",
    "                if(boxes_match(box[0],box1)):\n",
    "                    match_list.append(box_index)\n",
    "                    \n",
    "            elif(box.shape[0]==8):\n",
    "                \n",
    "                if(boxes_match(box,box1)):\n",
    "                    \n",
    "                    match_list.append(box_index)\n",
    "        box_index+=1\n",
    "    \n",
    "    if(len(match_list)== 0):\n",
    "        return index\n",
    "    max_match_index = 0\n",
    "    max_match_iou = 0\n",
    "    for i in match_list:\n",
    "        cur_iou = iou(box1[3:7],list_boxes[i][3:7])\n",
    "        \n",
    "        if(cur_iou>max_match_iou):\n",
    "            max_match_iou=cur_iou\n",
    "            max_match_index =i\n",
    "    list_boxes[max_match_index][7] = 1\n",
    "    box1[7] = 1\n",
    "    \n",
    "    return max_match_index\n",
    "\n",
    "def box_in_list(box1,list_boxes):\n",
    "    if(len(list_boxes)==0):\n",
    "        return -1\n",
    "    index=0\n",
    "    for box in list_boxes:\n",
    "        if(box.shape[0]>0):\n",
    "            if(box.shape[0]==1):\n",
    "                if(boxes_match(box[0],box1)):\n",
    "                    return index\n",
    "            elif(box.shape[0]==7):\n",
    "                if(boxes_match(box,box1)):\n",
    "                    return index\n",
    "        index+=1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Flow of center point and manual confidence update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "464\n",
      "prev_count   current_count   entered   exited\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-edefaef2dae8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0minput_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0minput_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mconfidence_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "import argparse\n",
    "import datetime\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "from bounding_box_utils.bounding_box_utils import iou\n",
    "no_tracking_res = [] \n",
    "tracking_res = []\n",
    "vs = cv2.VideoCapture('../modd.avi')\n",
    "# initialize the first frame in the video stream\n",
    "firstFrame = None\n",
    "frameCount =0\n",
    "step = 0.05\n",
    "total_objects_no_tracking=0\n",
    "total_objects_tracking=0\n",
    "frame_width = int(vs.get(3))\n",
    "frame_height = int(vs.get(4))\n",
    "print(frame_width)\n",
    "print(frame_height)\n",
    "preds = []\n",
    "pred = None\n",
    "tracks=[]\n",
    "out_tracking = cv2.VideoWriter('mod_ssd_tracking_all.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 30, (frame_width,frame_height))\n",
    "started = False\n",
    "# your code\n",
    "print(\"prev_count   current_count   entered   exited\")\n",
    "multiplier=0\n",
    "cc=0\n",
    "prev_frame=None\n",
    "while frameCount<641:\n",
    "    # grab the current frame and initialize the occupied/unoccupied\n",
    "    # text\n",
    "    frame = vs.read()[1]\n",
    "    \n",
    " \n",
    "    # if the frame could not be grabbed, then we have reached the end\n",
    "    # of the video\n",
    "    if frame is None:\n",
    "        break\n",
    "    frameCount+=1\n",
    "    if(frameCount<0):\n",
    "        continue\n",
    "    elif started==False:\n",
    "        start_time = time.time()\n",
    "        started=True\n",
    "    # resize the frame, convert it to grayscale, and blur it\n",
    "    cv2.imwrite('temp.jpg',frame)\n",
    "    img = image.load_img('temp.jpg', target_size=(img_height, img_width))\n",
    "    img = image.img_to_array(img)\n",
    "    input_images = []\n",
    "    \n",
    "    input_images.append(img)\n",
    "    input_images = np.array(input_images)\n",
    "    y_pred = model.predict(input_images)\n",
    "    \n",
    "    confidence_threshold = 0.1\n",
    "    current_pred = []\n",
    "    y_pred_thresh = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n",
    "    f = frame\n",
    "    \n",
    "        \n",
    "    # loop over the contours\n",
    "    preds = []\n",
    "    for box in y_pred_thresh[0]:\n",
    "        if(box[0]!=4):\n",
    "            continue\n",
    "       \n",
    "        preds.append(np.insert(box,0,0))\n",
    "    \n",
    "    #print(len(prev_pred),len(temp),len(entered_pred),len(exited_pred))\n",
    "    print(tracks)\n",
    "    for pred in preds:\n",
    "        \n",
    "        if(box_matches_list(pred,tracks)):\n",
    "            \n",
    "                \n",
    "            index = box_in_list(pred,tracks)\n",
    "            multiplier = tracks[index][0]\n",
    "            org_conf =tracks[index][2]\n",
    "            if(multiplier<0):\n",
    "                multiplier=1\n",
    "            else:\n",
    "                multiplier +=1\n",
    "            \n",
    "             \n",
    "            del tracks[index]\n",
    "            \n",
    "            pred[2] += multiplier*step\n",
    "            \n",
    "            pred[0] = multiplier\n",
    "            #print(pred)\n",
    "            tracks.append(pred)\n",
    "        else:\n",
    "            \n",
    "            temp_pred = np.copy(pred)\n",
    "           \n",
    "            multiplier = 1\n",
    "            \n",
    "          \n",
    "            temp_pred[0] = multiplier\n",
    "            tracks.append(temp_pred)\n",
    "         \n",
    "            cc = frameCount\n",
    "    print(tracks)\n",
    "    for track in tracks:\n",
    "        if(not box_matches_list(track,preds)):\n",
    "            #print('not found in current frame')\n",
    "            track[0]-=1\n",
    "            #print('multiplier decreased to ',track[0])\n",
    "            track[2]+=(step*track[0])\n",
    "            frame_grey = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "            prev_frame_grey = cv2.cvtColor(prev_frame,cv2.COLOR_BGR2GRAY)\n",
    "            flow = cv2.calcOpticalFlowFarneback(prev_frame_grey,frame_grey, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            xmin = int(track[3] * frame.shape[1] / img_width)\n",
    "            ymin = int(track[4] * frame.shape[0] / img_height)\n",
    "            xmax =int(track[5] * frame.shape[1] / img_width)\n",
    "            ymax =int(track[6] * frame.shape[0] / img_height)\n",
    "            center_flow = flow[np.minimum(int((xmin+xmax)/2),flow.shape[0]-1),np.minimum(int((ymin+ymax)/2),flow.shape[1]-1)]\n",
    "            #print('item is')\n",
    "            #print(track)\n",
    "            #print('center in image is')\n",
    "            #print(int((xmin+xmax)/2),int((ymin+ymax)/2))\n",
    "            #print('flow in image is')\n",
    "           # print(center_flow)\n",
    "            center_flow_min_x = int(center_flow[0] * img_width / frame.shape[1])\n",
    "            center_flow_min_y = int(center_flow[1] * img_height / frame.shape[0])\n",
    "            center_min_x = int((xmin+xmax)/2*img_width/frame.shape[1])\n",
    "            center_min_y = int((ymin+ymax)/2*img_height/frame.shape[0])\n",
    "            #print('center in min image is')\n",
    "            #print(center_min_x,center_min_y)\n",
    "            track[3] += center_flow_min_x\n",
    "            track[5]+=center_flow_min_x\n",
    "            track[4]+=center_flow_min_y\n",
    "            track[6]+=center_flow_min_y\n",
    "            #print('flow in min image is')\n",
    "            #print(center_flow)\n",
    "            \n",
    "    to_display = [track for track in tracks if track[2]>0.4]\n",
    "    #print(tracks)\n",
    "    for box in to_display:\n",
    "    # Transform the predicted bounding boxes for the 512x512 image to the original image dimensions.\n",
    "    \n",
    "        xmin = int(box[3] * frame.shape[1] / img_width)\n",
    "        ymin = int(box[4] * frame.shape[0] / img_height)\n",
    "        xmax =int(box[5] * frame.shape[1] / img_width)\n",
    "        ymax =int(box[6] * frame.shape[0] / img_height)\n",
    "        cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax),int(ymax)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame,'{}: {:.2f}'.format(classes[int(box[1])], box[2]), (xmin, ymin),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        total_objects_tracking+=1\n",
    "        tracking_res.append({\"image_id\" : frameCount, \"category_id\" : 1, \"bbox\" : [float(xmin),float(ymin),float(xmax-xmin),float(ymax-ymin)], \"score\" : int(box[1])})\n",
    "    \n",
    "    # if the `q` key is pressed, break from the lop\n",
    "    \n",
    "    out_tracking.write(frame)\n",
    "    prev_frame=frame\n",
    "# cleanup the camera and close any open windows\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "out_tracking.release()\n",
    "\n",
    "print(total_objects_tracking)\n",
    "with open('res_tracking.json', 'w') as outfile:  \n",
    "    json.dump(tracking_res, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_no_tracking.json', 'w') as outfile:  \n",
    "    json.dump(no_tracking_res, outfile)\n",
    "with open('res_tracking.json', 'w') as outfile:  \n",
    "    json.dump(tracking_res, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Key-Point Flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "480\n",
      "prev_count   current_count   entered   exited\n",
      "604.4723992347717\n",
      "1080\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "import argparse\n",
    "import datetime\n",
    "import imutils\n",
    "import time\n",
    "import cv2 as cv\n",
    "import time\n",
    "import json\n",
    "from bounding_box_utils.bounding_box_utils import iou\n",
    "feature_params = dict( maxCorners = 25,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "no_tracking_res = [] \n",
    "tracking_res = []\n",
    "vs = cv.VideoCapture('../1.avi')\n",
    "# initialize the first frame in the video stream\n",
    "firstFrame = None\n",
    "frameCount =0\n",
    "step = 0.05\n",
    "total_objects_no_tracking=0\n",
    "total_objects_tracking=0\n",
    "frame_width = int(vs.get(3))\n",
    "frame_height = int(vs.get(4))\n",
    "print(frame_width)\n",
    "print(frame_height)\n",
    "preds = []\n",
    "pred = None\n",
    "tracks=[]\n",
    "out_tracking = cv.VideoWriter('1_tracking_keypoi.avi',cv.VideoWriter_fourcc('M','J','P','G'), 30, (frame_width,frame_height))\n",
    "started = False\n",
    "# your code\n",
    "print(\"prev_count   current_count   entered   exited\")\n",
    "multiplier=0\n",
    "cc=0\n",
    "prev_frame=None\n",
    "while frameCount<540:\n",
    "    # grab the current frame and initialize the occupied/unoccupied\n",
    "    # text\n",
    "    frame = vs.read()[1]\n",
    "    \n",
    " \n",
    "    # if the frame could not be grabbed, then we have reached the end\n",
    "    # of the video\n",
    "    if frame is None:\n",
    "        break\n",
    "    frameCount+=1\n",
    "    if(frameCount<0):\n",
    "        continue\n",
    "    elif started==False:\n",
    "        start_time = time.time()\n",
    "        started=True\n",
    "    # resize the frame, convert it to grayscale, and blur it\n",
    "    cv.imwrite('temp.jpg',frame)\n",
    "    img = image.load_img('temp.jpg', target_size=(img_height, img_width))\n",
    "    img = image.img_to_array(img)\n",
    "    input_images = []\n",
    "    \n",
    "    input_images.append(img)\n",
    "    input_images = np.array(input_images)\n",
    "    y_pred = model.predict(input_images)\n",
    "    \n",
    "    confidence_threshold = 0.1\n",
    "    current_pred = []\n",
    "    y_pred_thresh = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n",
    "    f = frame\n",
    "    \n",
    "        \n",
    "    # loop over the contours\n",
    "    preds = []\n",
    "    for box in y_pred_thresh[0]:\n",
    "        if(box[0]!=4):\n",
    "            continue\n",
    "        temp_pred = np.insert(box,0,0)\n",
    "        temp_pred=np.insert(temp_pred,7,0)\n",
    "        preds.append(temp_pred)\n",
    "    for track in tracks:\n",
    "        track[7]=0\n",
    "    #print(len(prev_pred),len(temp),len(entered_pred),len(exited_pred))\n",
    "    \n",
    "    for pred in preds:\n",
    "        index = box_matches_list(pred,tracks)\n",
    "        if(index>-1):\n",
    "            \n",
    "            multiplier = tracks[index][0]\n",
    "            org_conf =tracks[index][2]\n",
    "            if(multiplier<0):\n",
    "                multiplier=1\n",
    "            else:\n",
    "                multiplier +=1\n",
    "            \n",
    "             \n",
    "            del tracks[index]\n",
    "            \n",
    "            pred[2] += multiplier*step\n",
    "            \n",
    "            pred[0] = multiplier\n",
    "            #print(pred)\n",
    "            tracks.append(pred)\n",
    "        else:\n",
    "            \n",
    "            temp_pred = np.copy(pred)\n",
    "           \n",
    "            multiplier = 1\n",
    "            \n",
    "          \n",
    "            temp_pred[0] = multiplier\n",
    "            tracks.append(temp_pred)\n",
    "         \n",
    "            cc = frameCount\n",
    "\n",
    "    for track in tracks:\n",
    "        if(box_matches_list(track,preds)==-1):\n",
    "            #print('not found in current frame')\n",
    "            track[0]-=1\n",
    "            #print('multiplier decreased to ',track[0])\n",
    "            track[2]+=(step*track[0])\n",
    "            frame_grey = cv.cvtColor(frame,cv.COLOR_BGR2GRAY)\n",
    "            prev_frame_grey = cv.cvtColor(prev_frame,cv.COLOR_BGR2GRAY)\n",
    "            mask = np.zeros(frame_grey.shape, dtype = \"uint8\")\n",
    "\n",
    "\n",
    "            cv.rectangle(mask, (track[3], track[4]), (track[5], track[6]), (255, 255, 255), -1)\n",
    "            p0 = cv.goodFeaturesToTrack(prev_frame_grey, mask = mask, **feature_params)\n",
    "            if(not p0 is None ):\n",
    "                p1, st, err = cv.calcOpticalFlowPyrLK(prev_frame_grey, frame_grey, p0, None, **lk_params)\n",
    "                \n",
    "                average_flow = np.average(p1-p0,0)[0]\n",
    "            else:\n",
    "                average_flow=[0,0]\n",
    "            xmin = int(track[3] * frame.shape[1] / img_width)\n",
    "            ymin = int(track[4] * frame.shape[0] / img_height)\n",
    "            xmax =int(track[5] * frame.shape[1] / img_width)\n",
    "            ymax =int(track[6] * frame.shape[0] / img_height)\n",
    "            flow_x_box_coords = int(average_flow[0] * img_width / frame.shape[1])\n",
    "            flow_y_box_coords = int(average_flow[1] * img_height / frame.shape[0])\n",
    "            track[3] += flow_x_box_coords\n",
    "            track[5]+=flow_x_box_coords\n",
    "            track[4]+=flow_y_box_coords\n",
    "            track[6]+=flow_y_box_coords\n",
    "            \n",
    "            \n",
    "    to_display = [track for track in tracks if track[2]>0.4]\n",
    "    #print(tracks)\n",
    "    for box in to_display:\n",
    "    # Transform the predicted bounding boxes for the 512x512 image to the original image dimensions.\n",
    "    \n",
    "        xmin = int(box[3] * frame.shape[1] / img_width)\n",
    "        ymin = int(box[4] * frame.shape[0] / img_height)\n",
    "        xmax =int(box[5] * frame.shape[1] / img_width)\n",
    "        ymax =int(box[6] * frame.shape[0] / img_height)\n",
    "        cv.rectangle(frame, (int(xmin), int(ymin)), (int(xmax),int(ymax)), (0, 255, 0), 2)\n",
    "        cv.putText(frame,'{}: {:.2f}'.format(classes[int(box[1])], box[2]), (xmin, ymin),cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        total_objects_tracking+=1\n",
    "        tracking_res.append({\"image_id\" : frameCount, \"category_id\" : 1, \"bbox\" : [float(xmin),float(ymin),float(xmax-xmin),float(ymax-ymin)], \"score\" : int(box[1])})\n",
    "    #print('preds%d'%len(preds))\n",
    "    #print('tracks%d'%len(tracks))\n",
    "    #print('displayed%d'%len(to_display))\n",
    "    # if the `q` key is pressed, break from the lop\n",
    "    #plt.figure()\n",
    "    #plt.imshow(frame)\n",
    "    out_tracking.write(frame)\n",
    "    prev_frame=frame\n",
    "# cleanup the camera and close any open windows\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "vs.release()\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "out_tracking.release()\n",
    "\n",
    "print(total_objects_tracking)\n",
    "with open('1_res_keypoint.json', 'w') as outfile:  \n",
    "    json.dump(tracking_res, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "464\n",
      "prev_count   current_count   entered   exited\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sqrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-eda08d052458>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkalman\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mprocess_noise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkalman\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessNoiseCov\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkalman\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransitionMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprocess_noise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;31m#print(state)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sqrt' is not defined"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "import argparse\n",
    "import datetime\n",
    "import imutils\n",
    "import time\n",
    "import cv2 as cv\n",
    "import time\n",
    "import json\n",
    "from bounding_box_utils.bounding_box_utils import iou\n",
    "feature_params = dict( maxCorners = 25,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "no_tracking_res = [] \n",
    "tracking_res = []\n",
    "vs = cv.VideoCapture('../modd.avi')\n",
    "# initialize the first frame in the video stream\n",
    "firstFrame = None\n",
    "frameCount =0\n",
    "step = 0.05\n",
    "total_objects_no_tracking=0\n",
    "total_objects_tracking=0\n",
    "frame_width = int(vs.get(3))\n",
    "frame_height = int(vs.get(4))\n",
    "print(frame_width)\n",
    "print(frame_height)\n",
    "preds = []\n",
    "pred = None\n",
    "tracks=[]\n",
    "\n",
    "out_tracking = cv.VideoWriter('1_tracking_keypoi.avi',cv.VideoWriter_fourcc('M','J','P','G'), 30, (frame_width,frame_height))\n",
    "started = False\n",
    "# your code\n",
    "print(\"prev_count   current_count   entered   exited\")\n",
    "multiplier=0\n",
    "cc=0\n",
    "prev_frame=None\n",
    "\n",
    "while frameCount<540:\n",
    "    # grab the current frame and initialize the occupied/unoccupied\n",
    "    # text\n",
    "    frame = vs.read()[1]\n",
    "    \n",
    " \n",
    "    # if the frame could not be grabbed, then we have reached the end\n",
    "    # of the video\n",
    "    if frame is None:\n",
    "        break\n",
    "    frameCount+=1\n",
    "    if(frameCount<150):\n",
    "        continue\n",
    "    elif started==False:\n",
    "        start_time = time.time()\n",
    "        started=True\n",
    "    # resize the frame, convert it to grayscale, and blur it\n",
    "    cv.imwrite('temp.jpg',frame)\n",
    "    img = image.load_img('temp.jpg', target_size=(img_height, img_width))\n",
    "    img = image.img_to_array(img)\n",
    "    input_images = []\n",
    "    \n",
    "    input_images.append(img)\n",
    "    input_images = np.array(input_images)\n",
    "    y_pred = model.predict(input_images)\n",
    "    \n",
    "    confidence_threshold = 0.1\n",
    "    current_pred = []\n",
    "    y_pred_thresh = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n",
    "    f = frame\n",
    "    \n",
    "    prediction = kalman.predict()\n",
    "    process_noise = sqrt(kalman.processNoiseCov[0,0]) * np.random.randn(4, 1)\n",
    "    state = np.dot(kalman.transitionMatrix, state) + process_noise\n",
    "    #print(state)\n",
    "    # loop over the contours\n",
    "    preds = []\n",
    "    for box in y_pred_thresh[0]:\n",
    "        if(box[0]!=4):\n",
    "            continue\n",
    "        temp_pred = np.insert(box,0,0)\n",
    "        temp_pred=np.insert(temp_pred,7,0)\n",
    "        preds.append(temp_pred)\n",
    "        \n",
    "        print('correcting with ',box[2:6])\n",
    "        kalman.correct(box[2:6])\n",
    "        \n",
    "    for track in tracks:\n",
    "        track[7]=0\n",
    "    #print(len(prev_pred),len(temp),len(entered_pred),len(exited_pred))\n",
    "    \n",
    "    for pred in preds:\n",
    "        index = box_matches_list(pred,tracks)\n",
    "        if(index>-1):\n",
    "            \n",
    "            multiplier = tracks[index][0]\n",
    "            org_conf =tracks[index][2]\n",
    "            if(multiplier<0):\n",
    "                multiplier=1\n",
    "            else:\n",
    "                multiplier +=1\n",
    "            \n",
    "             \n",
    "            del tracks[index]\n",
    "            \n",
    "            pred[2] += multiplier*step\n",
    "            \n",
    "            pred[0] = multiplier\n",
    "            #print(pred)\n",
    "            tracks.append(pred)\n",
    "        else:\n",
    "            \n",
    "            temp_pred = np.copy(pred)\n",
    "           \n",
    "            multiplier = 1\n",
    "            \n",
    "          \n",
    "            temp_pred[0] = multiplier\n",
    "            tracks.append(temp_pred)\n",
    "         \n",
    "            cc = frameCount\n",
    "    print('prediction:',kalman.predict())\n",
    "    for track in tracks:\n",
    "        if(tracks[7]==0 and box_matches_list(track,preds)==-1):\n",
    "            #print('not found in current frame')\n",
    "            track[0]-=1\n",
    "            #print('multiplier decreased to ',track[0])\n",
    "            track[2]+=(step*track[0])\n",
    "            frame_grey = cv.cvtColor(frame,cv.COLOR_BGR2GRAY)\n",
    "            prev_frame_grey = cv.cvtColor(prev_frame,cv.COLOR_BGR2GRAY)\n",
    "            mask = np.zeros(frame_grey.shape, dtype = \"uint8\")\n",
    "\n",
    "\n",
    "            cv.rectangle(mask, (track[3], track[4]), (track[5], track[6]), (255, 255, 255), -1)\n",
    "            p0 = cv.goodFeaturesToTrack(prev_frame_grey, mask = mask, **feature_params)\n",
    "            if(not p0 is None ):\n",
    "                p1, st, err = cv.calcOpticalFlowPyrLK(prev_frame_grey, frame_grey, p0, None, **lk_params)\n",
    "                \n",
    "                average_flow = np.average(p1-p0,0)[0]\n",
    "            else:\n",
    "                average_flow=[0,0]\n",
    "            xmin = int(track[3] * frame.shape[1] / img_width)\n",
    "            ymin = int(track[4] * frame.shape[0] / img_height)\n",
    "            xmax =int(track[5] * frame.shape[1] / img_width)\n",
    "            ymax =int(track[6] * frame.shape[0] / img_height)\n",
    "            flow_x_box_coords = int(average_flow[0] * img_width / frame.shape[1])\n",
    "            flow_y_box_coords = int(average_flow[1] * img_height / frame.shape[0])\n",
    "            track[3] += flow_x_box_coords\n",
    "            track[5]+=flow_x_box_coords\n",
    "            track[4]+=flow_y_box_coords\n",
    "            track[6]+=flow_y_box_coords\n",
    "            \n",
    "            \n",
    "    to_display = [track for track in tracks if track[2]>0.4]\n",
    "    #print(tracks)\n",
    "    for box in to_display:\n",
    "    # Transform the predicted bounding boxes for the 512x512 image to the original image dimensions.\n",
    "    \n",
    "        xmin = int(box[3] * frame.shape[1] / img_width)\n",
    "        ymin = int(box[4] * frame.shape[0] / img_height)\n",
    "        xmax =int(box[5] * frame.shape[1] / img_width)\n",
    "        ymax =int(box[6] * frame.shape[0] / img_height)\n",
    "        cv.rectangle(frame, (int(xmin), int(ymin)), (int(xmax),int(ymax)), (0, 255, 0), 2)\n",
    "        cv.putText(frame,'{}: {:.2f}'.format(classes[int(box[1])], box[2]), (xmin, ymin),cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        total_objects_tracking+=1\n",
    "        tracking_res.append({\"image_id\" : frameCount, \"category_id\" : 1, \"bbox\" : [float(xmin),float(ymin),float(xmax-xmin),float(ymax-ymin)], \"score\" : int(box[1])})\n",
    "    #print('preds%d'%len(preds))\n",
    "    #print('tracks%d'%len(tracks))\n",
    "    #print('displayed%d'%len(to_display))\n",
    "    # if the `q` key is pressed, break from the lop\n",
    "    #plt.figure()\n",
    "    #plt.imshow(frame)\n",
    "    out_tracking.write(frame)\n",
    "    prev_frame=frame\n",
    "# cleanup the camera and close any open windows\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "vs.release()\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "out_tracking.release()\n",
    "\n",
    "print(total_objects_tracking)\n",
    "with open('1_res_keypoint.json', 'w') as outfile:  \n",
    "    json.dump(tracking_res, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "   Tracking of rotating point.\n",
    "   Rotation speed is constant.\n",
    "   Both state and measurements vectors are 1D (a point angle),\n",
    "   Measurement is the real point angle + gaussian noise.\n",
    "   The real and the estimated points are connected with yellow line segment,\n",
    "   the real and the measured points are connected with red line segment.\n",
    "   (if Kalman filter works correctly,\n",
    "    the yellow segment should be shorter than the red one).\n",
    "   Pressing any key (except ESC) will reset the tracking with a different speed.\n",
    "   Pressing ESC will stop the program.\n",
    "\"\"\"\n",
    "# Python 2/3 compatibility\n",
    "import sys\n",
    "PY3 = sys.version_info[0] == 3\n",
    "\n",
    "if PY3:\n",
    "    long = int\n",
    "\n",
    "import cv2 as cv\n",
    "from math import cos, sin, sqrt\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    img_height = 500\n",
    "    img_width = 500\n",
    "    kalman = cv.KalmanFilter(4, 1, 0)\n",
    "\n",
    "    code = long(-1)\n",
    "\n",
    "    cv.namedWindow(\"Kalman\")\n",
    "\n",
    "    while True:\n",
    "        state = 0.1 * np.random.randn(4, 1)\n",
    "\n",
    "        kalman.transitionMatrix = np.array([[1., 1.,1.,1.], [0., 1.,1.,1.],[0,1,1,0],[1,0,0,1]])\n",
    "        kalman.measurementMatrix = 1. * np.ones((1, 4))\n",
    "        kalman.processNoiseCov = 1e-5 * np.eye(4)\n",
    "        kalman.measurementNoiseCov = 1e-1 * np.ones((1, 1))\n",
    "        kalman.errorCovPost = 1. * np.ones((4, 4))\n",
    "        kalman.statePost = 0.1 * np.random.randn(4, 1)\n",
    "\n",
    "        while True:\n",
    "            def calc_point(angle):\n",
    "                return (np.around(img_width/2 + img_width/3*cos(angle), 0).astype(int),\n",
    "                        np.around(img_height/2 - img_width/3*sin(angle), 1).astype(int))\n",
    "\n",
    "            state_angle = state[0, 0]\n",
    "            state_pt = calc_point(state_angle)\n",
    "\n",
    "            prediction = kalman.predict()\n",
    "            predict_angle = prediction[0, 0]\n",
    "            predict_pt = calc_point(predict_angle)\n",
    "\n",
    "            measurement = kalman.measurementNoiseCov * np.random.randn(1, 1)\n",
    "\n",
    "            # generate measurement\n",
    "            measurement = np.dot(kalman.measurementMatrix, state) + measurement\n",
    "\n",
    "            measurement_angle = measurement[0, 0]\n",
    "            measurement_pt = calc_point(measurement_angle)\n",
    "\n",
    "            # plot points\n",
    "            def draw_cross(center, color, d):\n",
    "                cv.line(img,\n",
    "                         (center[0] - d, center[1] - d), (center[0] + d, center[1] + d),\n",
    "                         color, 1, cv.LINE_AA, 0)\n",
    "                cv.line(img,\n",
    "                         (center[0] + d, center[1] - d), (center[0] - d, center[1] + d),\n",
    "                         color, 1, cv.LINE_AA, 0)\n",
    "\n",
    "            img = np.zeros((img_height, img_width, 3), np.uint8)\n",
    "            draw_cross(np.int32(state_pt), (255, 255, 255), 3)\n",
    "            draw_cross(np.int32(measurement_pt), (0, 0, 255), 3)\n",
    "            draw_cross(np.int32(predict_pt), (0, 255, 0), 3)\n",
    "\n",
    "            cv.line(img, state_pt, measurement_pt, (0, 0, 255), 3, cv.LINE_AA, 0)\n",
    "            cv.line(img, state_pt, predict_pt, (0, 255, 255), 3, cv.LINE_AA, 0)\n",
    "\n",
    "            kalman.correct(measurement)\n",
    "\n",
    "            process_noise = sqrt(kalman.processNoiseCov[0,0]) * np.random.randn(2, 1)\n",
    "            state = np.dot(kalman.transitionMatrix, state) + process_noise\n",
    "\n",
    "            cv.imshow(\"Kalman\", img)\n",
    "\n",
    "            code = cv.waitKey(100)\n",
    "            if code != -1:\n",
    "                break\n",
    "\n",
    "        if code in [27, ord('q'), ord('Q')]:\n",
    "            break\n",
    "\n",
    "    cv.destroyWindow(\"Kalman\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalman.correct??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
